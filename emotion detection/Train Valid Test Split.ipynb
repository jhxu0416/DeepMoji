{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"data/clean/positive.csv\", encoding= \"utf-8\")\n",
    "positive = list(positive[\"text\"])\n",
    "slightly_positive = pd.read_csv(\"data/clean/slightly_positive.csv\", encoding= \"utf-8\")\n",
    "slightly_positive = list(slightly_positive[\"text\"])\n",
    "slightly_negative = pd.read_csv(\"data/clean/slightly_negative.csv\", encoding= \"utf-8\")\n",
    "slightly_negative = list(slightly_negative[\"text\"])\n",
    "negative = pd.read_csv(\"data/clean/negative.csv\", encoding= \"utf-8\")\n",
    "negative = list(negative[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use DeepMoji to encode texts into emotional feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import example_helper\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from deepmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from deepmoji.model_def import deepmoji_feature_encoding\n",
    "from deepmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /Users/xjh/Github/DeepMoji/model/vocabulary.json\n"
     ]
    }
   ],
   "source": [
    "maxlen = 30\n",
    "batch_size = 32\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/xjh/Github/DeepMoji/model/deepmoji_weights.hdf5.\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:488: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:2585: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1370: calling reduce_all_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1204: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Loading weights for embedding\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From //anaconda3/envs/DeepMoji/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "Loading weights for bi_lstm_0\n",
      "Loading weights for bi_lstm_1\n",
      "Loading weights for attlayer\n",
      "Ignoring weights for softmax\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 256)      12800000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 30, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 30, 1024)     3149824     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       (None, 30, 1024)     6295552     bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 2304)     0           bi_lstm_1[0][0]                  \n",
      "                                                                 bi_lstm_0[0][0]                  \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attlayer (AttentionWeightedAver (None, 2304)         2304        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 22,247,680\n",
      "Trainable params: 22,247,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "model = deepmoji_feature_encoding(maxlen, PRETRAINED_PATH)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding texts..\n",
      "Positive shape (31549, 2304)\n",
      "Positive done\n",
      "Slightly positive shape (34806, 2304)\n",
      "Slightly positive done\n",
      "Slightly negative shape (28486, 2304)\n",
      "Slightly negative done\n",
      "Negative shape (37283, 2304)\n",
      "Negative done\n"
     ]
    }
   ],
   "source": [
    "print('Encoding texts..')\n",
    "tokenized_positive, _, _ = st.tokenize_sentences(positive)\n",
    "encoding_positive = model.predict(tokenized_positive)\n",
    "print(\"Positive shape {}\".format(encoding_positive.shape))\n",
    "print(\"Positive done\")\n",
    "\n",
    "tokenized_slightly_positive, _, _ = st.tokenize_sentences(slightly_positive)\n",
    "encoding_slightly_positive = model.predict(tokenized_slightly_positive)\n",
    "print(\"Slightly positive shape {}\".format(encoding_slightly_positive.shape))\n",
    "print(\"Slightly positive done\")\n",
    "\n",
    "tokenized_slightly_negative, _, _ = st.tokenize_sentences(slightly_negative)\n",
    "encoding_slightly_negative = model.predict(tokenized_slightly_negative)\n",
    "print(\"Slightly negative shape {}\".format(encoding_slightly_negative.shape))\n",
    "print(\"Slightly negative done\")\n",
    "\n",
    "tokenized_negative, _, _ = st.tokenize_sentences(negative)\n",
    "encoding_negative = model.predict(tokenized_negative)\n",
    "print(\"Negative shape {}\".format(encoding_negative.shape))\n",
    "print(\"Negative done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "valid_size = 0.1\n",
    "test_size = 0.1\n",
    "random_state = 888\n",
    "total_labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(input_matrix, label, train_size, valid_size, test_size, \n",
    "                           random_state, total_labels):\n",
    "    n = input_matrix.shape[0]\n",
    "    X_train_idx, X_test_idx = train_test_split(list(range(n)), \n",
    "                                               test_size = valid_size + test_size, \n",
    "                                               random_state = random_state)\n",
    "    X_valid_idx, X_test_idx = train_test_split(X_test_idx, \n",
    "                                               test_size = test_size /(valid_size + test_size), \n",
    "                                               random_state = random_state)\n",
    "    # training set\n",
    "    X_train = input_matrix[X_train_idx]\n",
    "    Y_train = np.eye(total_labels, dtype=int)[np.repeat(label, len(X_train_idx))]\n",
    "    \n",
    "    # validation set\n",
    "    X_valid = input_matrix[X_valid_idx]\n",
    "    Y_valid = np.eye(total_labels, dtype=int)[np.repeat(label, len(X_valid_idx))]\n",
    "    \n",
    "    # test set\n",
    "    X_test = input_matrix[X_test_idx]\n",
    "    Y_test = np.eye(total_labels, dtype=int)[np.repeat(label, len(X_test_idx))]\n",
    "\n",
    "    return {\"X_train\": X_train, \n",
    "            \"Y_train\": Y_train, \n",
    "            \"X_valid\": X_valid, \n",
    "            \"Y_valid\": Y_valid, \n",
    "            \"X_test\": X_test, \n",
    "            \"Y_test\": Y_test,\n",
    "            \"train_idx\": X_train_idx,\n",
    "            \"valid_idx\": X_valid_idx,\n",
    "            \"test_idx\": X_test_idx}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data_0 = train_valid_test_split(encoding_positive, \n",
    "                                         0, train_size, valid_size, test_size, \n",
    "                                         random_state, total_labels)\n",
    "splitted_data_1 = train_valid_test_split(encoding_slightly_positive, \n",
    "                                         1, train_size, valid_size, test_size, \n",
    "                                         random_state, total_labels)\n",
    "splitted_data_2 = train_valid_test_split(encoding_slightly_negative, \n",
    "                                         2, train_size, valid_size, test_size, \n",
    "                                         random_state, total_labels)\n",
    "splitted_data_3 = train_valid_test_split(encoding_negative, \n",
    "                                         3, train_size, valid_size, test_size, \n",
    "                                         random_state, total_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((splitted_data_0.get(\"X_train\"),\n",
    "                             splitted_data_1.get(\"X_train\"),\n",
    "                             splitted_data_2.get(\"X_train\"),\n",
    "                             splitted_data_3.get(\"X_train\")), axis=0)\n",
    "train_label = np.concatenate((splitted_data_0.get(\"Y_train\"),\n",
    "                              splitted_data_1.get(\"Y_train\"),\n",
    "                              splitted_data_2.get(\"Y_train\"),\n",
    "                              splitted_data_3.get(\"Y_train\")), axis=0)\n",
    "train_index = np.concatenate((np.array(list(map(lambda x: [x,0], splitted_data_0.get(\"train_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,1], splitted_data_1.get(\"train_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,2], splitted_data_2.get(\"train_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,3], splitted_data_3.get(\"train_idx\"))))), axis=0)\n",
    "np.save(\"data/train_data.npy\", train_data)\n",
    "np.save(\"data/train_label.npy\", train_label)\n",
    "np.save(\"data/train_index.npy\", train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save valid data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = np.concatenate((splitted_data_0.get(\"X_valid\"),\n",
    "                             splitted_data_1.get(\"X_valid\"),\n",
    "                             splitted_data_2.get(\"X_valid\"),\n",
    "                             splitted_data_3.get(\"X_valid\")), axis=0)\n",
    "valid_label = np.concatenate((splitted_data_0.get(\"Y_valid\"),\n",
    "                              splitted_data_1.get(\"Y_valid\"),\n",
    "                              splitted_data_2.get(\"Y_valid\"),\n",
    "                              splitted_data_3.get(\"Y_valid\")), axis=0)\n",
    "valid_index = np.concatenate((np.array(list(map(lambda x: [x,0], splitted_data_0.get(\"valid_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,1], splitted_data_1.get(\"valid_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,2], splitted_data_2.get(\"valid_idx\")))),\n",
    "                              np.array(list(map(lambda x: [x,3], splitted_data_3.get(\"valid_idx\"))))), axis=0)\n",
    "np.save(\"data/valid_data.npy\", valid_data)\n",
    "np.save(\"data/valid_label.npy\", valid_label)\n",
    "np.save(\"data/valid_index.npy\", valid_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate((splitted_data_0.get(\"X_test\"),\n",
    "                             splitted_data_1.get(\"X_test\"),\n",
    "                             splitted_data_2.get(\"X_test\"),\n",
    "                             splitted_data_3.get(\"X_test\")), axis=0)\n",
    "test_label = np.concatenate((splitted_data_0.get(\"Y_test\"),\n",
    "                              splitted_data_1.get(\"Y_test\"),\n",
    "                              splitted_data_2.get(\"Y_test\"),\n",
    "                              splitted_data_3.get(\"Y_test\")), axis=0)\n",
    "test_index = np.concatenate((np.array(list(map(lambda x: [x,0], splitted_data_0.get(\"train_idx\")))),\n",
    "                             np.array(list(map(lambda x: [x,1], splitted_data_1.get(\"train_idx\")))),\n",
    "                             np.array(list(map(lambda x: [x,2], splitted_data_2.get(\"train_idx\")))),\n",
    "                             np.array(list(map(lambda x: [x,3], splitted_data_3.get(\"train_idx\"))))), axis=0)\n",
    "np.save(\"data/test_data.npy\", test_data)\n",
    "np.save(\"data/test_label.npy\", test_label)\n",
    "np.save(\"data/test_index.npy\", test_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepMoji)",
   "language": "python",
   "name": "deepmoji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
